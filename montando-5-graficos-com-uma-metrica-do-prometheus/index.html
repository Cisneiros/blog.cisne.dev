<!DOCTYPE html><html lang="pt-BR" class=" "><head><meta name="generator" content="React Static"/><title data-react-helmet="true">Montando 5 gráficos com uma métrica do Prometheus · Cisne.dev blog</title><meta charSet="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5, shrink-to-fit=no"/><meta data-react-helmet="true" property="og:type" content="article"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" name="twitter:creator" content="Cisneiros"/><meta data-react-helmet="true" property="og:title" content="Montando 5 gráficos com uma métrica do Prometheus · Cisne.dev blog"/><meta data-react-helmet="true" property="og:image" content="https://blog.cisne.dev/montando-5-graficos-com-uma-metrica-do-prometheus/social-image.png"/><meta data-react-helmet="true" name="twitter:title" content="Montando 5 gráficos com uma métrica do Prometheus · Cisne.dev blog"/><meta data-react-helmet="true" name="twitter:image" content="https://blog.cisne.dev/montando-5-graficos-com-uma-metrica-do-prometheus/social-image.png"/><link rel="preload" as="script" href="/templates/vendors~__react_static_root__/src/containers/post~__react_static_root__/src/containers/postList.207edc50.js"/><link rel="preload" as="script" href="/templates/__react_static_root__/src/containers/post~__react_static_root__/src/containers/postList.a04ea84b.js"/><link rel="preload" as="script" href="/templates/__react_static_root__/src/containers/post.d8dd52db.js"/><link rel="preload" as="script" href="/templates/vendors~main.272ae1d1.js"/><link rel="preload" as="script" href="/main.f19d4c14.js"/><link data-react-helmet="true" rel="stylesheet" href="//unpkg.com/dracula-prism/dist/css/dracula-prism.min.css"/><style data-styled="" data-styled-version="5.1.1">.CfxHf{max-width:960px;margin:0 auto;position:relative;}/*!sc*/
data-styled.g1[id="ui__Fit-prwz6b-0"]{content:"CfxHf,"}/*!sc*/
.iuDFSN{padding:1rem;color:var(--color-mid);text-align:center;font-size:1.5rem;}/*!sc*/
.iuDFSN a,.iuDFSN a:hover{-webkit-text-decoration:none;text-decoration:none;color:inherit;}/*!sc*/
data-styled.g2[id="Navigation__Container-sc-1yk0pqp-0"]{content:"iuDFSN,"}/*!sc*/
.izuwCZ{max-width:960px;margin:0 auto;position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g3[id="Navigation__NavFit-sc-1yk0pqp-1"]{content:"izuwCZ,"}/*!sc*/
.TctaC{width:2.5rem;height:1.2em;overflow:hidden;--default-rotation:-180deg;}/*!sc*/
@media (prefers-color-scheme:dark){.TctaC{--default-rotation:0;}}/*!sc*/
data-styled.g4[id="Navigation__BrightnessContainer-sc-1yk0pqp-2"]{content:"TctaC,"}/*!sc*/
.sLeDt{-webkit-transform:rotate(var(--default-rotation));-ms-transform:rotate(var(--default-rotation));transform:rotate(var(--default-rotation));-webkit-transition:-webkit-transform 0.3s ease-out;-webkit-transition:transform 0.3s ease-out;transition:transform 0.3s ease-out;}/*!sc*/
html.light .sLeDt{-webkit-transform:rotate(-180deg);-ms-transform:rotate(-180deg);transform:rotate(-180deg);}/*!sc*/
html.dark .sLeDt{-webkit-transform:rotate(0);-ms-transform:rotate(0);transform:rotate(0);}/*!sc*/
data-styled.g5[id="Navigation__BrightnessSpinner-sc-1yk0pqp-3"]{content:"sLeDt,"}/*!sc*/
.frsARl{background:none;border:none;cursor:pointer;}/*!sc*/
.frsARl:last-child{-webkit-transform:rotate(180deg);-ms-transform:rotate(180deg);transform:rotate(180deg);}/*!sc*/
data-styled.g7[id="Navigation__BrightnessIcon-sc-1yk0pqp-5"]{content:"frsARl,"}/*!sc*/
.gdHjSq{margin-top:3rem;padding:1rem;background:var(--color-foreground);color:var(--color-background);}/*!sc*/
.gdHjSq a{color:var(--text-inverse-color-1);}/*!sc*/
.gdHjSq a:hover{color:var(--text-inverse-color-3);}/*!sc*/
data-styled.g8[id="Footer__Container-b3q04c-0"]{content:"gdHjSq,"}/*!sc*/
.layJcq{max-width:960px;margin:0 auto;position:relative;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
data-styled.g9[id="Footer__FooterFit-b3q04c-1"]{content:"layJcq,"}/*!sc*/
*{box-sizing:border-box;margin:0;padding:0;border:0;font-size:inherit;font-weight:inherit;font-style:inherit;}/*!sc*/
:root{--color-1:#12c2e9;--color-2:#c471ed;--color-3:#f64f59;--color-1-darker:#0A7B94;--color-2-darker:#AE3CE7;--color-3-darker:#E90C1B;--color-light:#fefefe;--color-dark:#030303;--color-mid-dark:#747481;--color-mid-light:#818181;--font-weight-light:300;--font-weight-regular:400;--font-weight-heavy:500;--font-family-light:"HelveticaNeue-Light","Helvetica Neue Light",'Helvetica Neue',sans-serif;--font-family:'Helvetica Neue',sans-serif;}/*!sc*/
strong,h1,h2,h3,h4,h5,h6{font-family:var(--font-family);}/*!sc*/
html{font-family:var(--font-family-light);font-size:20px;line-height:1;color:var(--color-foreground);background-color:var(--color-background);text-rendering:optimizeLegibility;}/*!sc*/
@media (max-width:600px){html{font-size:16px;}}/*!sc*/
html,html.light{--color-background:var(--color-light);--color-foreground:var(--color-dark);--color-mid:var(--color-mid-dark);--text-color-1:var(--color-1-darker);--text-color-2:var(--color-2-darker);--text-color-3:var(--color-3-darker);--text-inverse-color-1:var(--color-1);--text-inverse-color-2:var(--color-2);--text-inverse-color-3:var(--color-3);}/*!sc*/
html.dark{--color-background:var(--color-dark);--color-foreground:var(--color-light);--color-mid:var(--color-mid-light);--text-color-1:var(--color-1);--text-color-2:var(--color-2);--text-color-3:var(--color-3);--text-inverse-color-1:var(--color-1-darker);--text-inverse-color-2:var(--color-2-darker);--text-inverse-color-3:var(--color-3-darker);}/*!sc*/
@media (prefers-color-scheme:dark){html{--color-background:var(--color-dark);--color-foreground:var(--color-light);--color-mid:var(--color-mid-light);}}/*!sc*/
body{font-size:1em;font-weight:var(--font-weight-light);}/*!sc*/
strong{font-weight:var(--font-weight-heavy);}/*!sc*/
em{font-style:italic;}/*!sc*/
main{padding:1rem;}/*!sc*/
hr{width:30%;margin:4rem auto;border:none;border-bottom:dotted 1px var(--color-1);}/*!sc*/
.pagination{text-align:center;}/*!sc*/
.pagination a{color:var(--text-color-1);}/*!sc*/
.pagination a:hover{color:var(--text-color-3);}/*!sc*/
a{color:var(--text-color-1);-webkit-text-decoration-color:#c471ed80;text-decoration-color:#c471ed80;-webkit-text-decoration-style:wavy;text-decoration-style:wavy;}/*!sc*/
a:hover{color:var(--text-color-3);-webkit-text-decoration-color:#12c2e980;text-decoration-color:#12c2e980;}/*!sc*/
.phone-mockup{max-width:15rem;margin:0 auto 1rem;position:relative;}/*!sc*/
.phone-mockup::before{content:'';width:100%;height:100%;position:absolute;pointer-events:none;background:url(/iphone-11-pro.png) no-repeat;background-size:contain;}/*!sc*/
.phone-mockup video,.phone-mockup img{width:100%;padding:9.3%;}/*!sc*/
data-styled.g14[id="sc-global-hFBrmQ1"]{content:"sc-global-hFBrmQ1,"}/*!sc*/
.tuaWG{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-box-pack:justify;-webkit-justify-content:space-between;-ms-flex-pack:justify;justify-content:space-between;}/*!sc*/
@media (max-width:600px){.tuaWG{-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;}}/*!sc*/
data-styled.g23[id="Poststyles__AfterPost-sc-12qb97v-0"]{content:"tuaWG,"}/*!sc*/
.gkeqUt{-webkit-flex:1;-ms-flex:1;flex:1;}/*!sc*/
@media (max-width:600px){.gkeqUt{text-align:center;margin-bottom:1rem;}}/*!sc*/
data-styled.g24[id="Poststyles__AfterPostChild-sc-12qb97v-1"]{content:"gkeqUt,"}/*!sc*/
.fAvgXR{-webkit-flex:1;-ms-flex:1;flex:1;text-align:right;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;}/*!sc*/
@media (max-width:600px){.fAvgXR{text-align:center;margin-bottom:1rem;}}/*!sc*/
data-styled.g25[id="Poststyles__Tags-sc-12qb97v-2"]{content:"fAvgXR,"}/*!sc*/
.kSZCZX::before{content:' #';color:var(--color-mid);}/*!sc*/
data-styled.g26[id="Poststyles__Tag-sc-12qb97v-3"]{content:"kSZCZX,"}/*!sc*/
.kyWzPB{-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto;}/*!sc*/
data-styled.g27[id="Poststyles__Container-sc-12qb97v-4"]{content:"kyWzPB,"}/*!sc*/
.hhBwVH{font-size:5rem;font-weight:var(--font-weight-regular);line-height:0.8;background:linear-gradient(var(--color-1),var(--color-2),var(--color-3));-webkit-text-fill-color:transparent;background-clip:text;-webkit-background-clip:text;padding-bottom:1rem;margin:1rem 0;}/*!sc*/
html.rainbow .hhBwVH{background-image:linear-gradient(124deg,#ff2400,#e81d1d,#e8b71d,#e3e81d,#1de840,#1ddde8,#2b1de8,#dd00f3,#dd00f3);-webkit-animation:hTUzwZ 9s ease infinite;animation:hTUzwZ 9s ease infinite;background-size:450% 450%;}/*!sc*/
@media (max-width:600px){.hhBwVH{font-size:3.5rem;}}/*!sc*/
.hhBwVH a{-webkit-text-decoration:none;text-decoration:none;}/*!sc*/
data-styled.g28[id="Poststyles__Title-sc-12qb97v-5"]{content:"hhBwVH,"}/*!sc*/
.fonWHZ{color:var(--color-mid);text-align:center;margin-bottom:2rem;}/*!sc*/
data-styled.g29[id="Poststyles__Meta-sc-12qb97v-6"]{content:"fonWHZ,"}/*!sc*/
.jhgzPU{line-height:1.6;}/*!sc*/
.jhgzPU h2{font-size:2.5rem;}/*!sc*/
.jhgzPU h3{font-size:2rem;}/*!sc*/
.jhgzPU h4{font-size:1.5rem;}/*!sc*/
.jhgzPU h2,.jhgzPU h3,.jhgzPU h4,.jhgzPU h5,.jhgzPU h6{-webkit-hyphens:auto;-moz-hyphens:auto;-ms-hyphens:auto;hyphens:auto;font-weight:var(--font-weight-regular);line-height:0.8;padding-bottom:1rem;margin-top:2rem;}/*!sc*/
.jhgzPU img{max-width:100%;}/*!sc*/
.jhgzPU pre.refractor{margin-top:0;margin-left:0;padding-left:1rem;border-left:solid 0.25rem var(--color-1);border-radius:0;font-size:0.8rem;overflow-wrap:normal;overflow-x:auto;background-color:var(--color-dark);color:var(--color-light);padding-top:0.5rem;padding-bottom:0.5rem;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;-webkit-hyphens:none;-moz-hyphens:none;-ms-hyphens:none;hyphens:none;}/*!sc*/
.jhgzPU pre.refractor::-webkit-scrollbar{width:0.25rem;height:0.25rem;}/*!sc*/
.jhgzPU pre.refractor code,.jhgzPU code{font-family:'Fira Code','Menlo',monospace;}/*!sc*/
.jhgzPU p code{color:var(--text-color-3);}/*!sc*/
.jhgzPU p code,.jhgzPU h1 code,.jhgzPU h2 code,.jhgzPU h3 code,.jhgzPU h4 code,.jhgzPU h5 code,.jhgzPU h6 code{font-size:0.9em;}/*!sc*/
.jhgzPU p,.jhgzPU pre,.jhgzPU ul,.jhgzPU ol,.jhgzPU table,.jhgzPU blockquote,.jhgzPU .live-code-container{margin-bottom:1rem;}/*!sc*/
.jhgzPU twitter-widget{margin-bottom:1rem !important;}/*!sc*/
.jhgzPU p:last-child,.jhgzPU pre:last-child,.jhgzPU ul:last-child,.jhgzPU ol:last-child,.jhgzPU table:last-child,.jhgzPU blockquote:last-child,.jhgzPU .live-code-container:last-child{margin-bottom:0;}/*!sc*/
.jhgzPU ul,.jhgzPU ol{list-style:none;margin-left:1rem;counter-reset:li;}/*!sc*/
.jhgzPU li{counter-increment:li;display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;}/*!sc*/
.jhgzPU ul li::before{content:'•';margin-right:0.5rem;color:var(--color-2);}/*!sc*/
.jhgzPU ol li::before{content:"."counter(li);margin-right:0.5rem;color:var(--color-2);font-weight:var(--font-weight-regular);width:1em;display:inline-block;margin-left:-0.5em;margin-right:0.5em;text-align:right;direction:rtl;}/*!sc*/
.jhgzPU table{margin-left:auto;margin-right:auto;border-spacing:0;font-size:0.9em;}/*!sc*/
.jhgzPU td,.jhgzPU th{border-bottom:solid 1px var(--color-2);padding:0.5rem;}/*!sc*/
.jhgzPU th{font-weight:var(--font-weight-heavy);}/*!sc*/
.jhgzPU tr:hover{background:var(--color-2);}/*!sc*/
.jhgzPU tr:last-of-type td{border-bottom:0;}/*!sc*/
.jhgzPU blockquote{padding-left:1rem;border-left:solid 0.25rem var(--color-3);color:var(--color-mid);}/*!sc*/
data-styled.g30[id="Poststyles__Content-sc-12qb97v-7"]{content:"jhgzPU,"}/*!sc*/
.jjKdZc{position:fixed;top:0;left:0;right:0;height:0.25rem;background-color:var(--color-1);-webkit-transition:opacity 0.25s ease-out;transition:opacity 0.25s ease-out;z-index:2;}/*!sc*/
data-styled.g31[id="post__ScrollTrackerContainer-gh0jqu-0"]{content:"jjKdZc,"}/*!sc*/
@-webkit-keyframes hTUzwZ{0%{background-position:0% 82%;}50%{background-position:100% 19%;}100%{background-position:0% 82%;}}/*!sc*/
@keyframes hTUzwZ{0%{background-position:0% 82%;}50%{background-position:100% 19%;}100%{background-position:0% 82%;}}/*!sc*/
data-styled.g32[id="sc-keyframes-hTUzwZ"]{content:"hTUzwZ,"}/*!sc*/
</style><link rel="alternate" type="application/rss+xml" title="RSS Feed" href="/feed.xml"/></head><body><div id="root"><nav class="Navigation__Container-sc-1yk0pqp-0 iuDFSN"><div class="ui__Fit-prwz6b-0 Navigation__NavFit-sc-1yk0pqp-1 izuwCZ"><p class="logo"><a href="/">Cisne.dev blog</a></p><div class="Navigation__BrightnessContainer-sc-1yk0pqp-2 TctaC"><div class="Navigation__BrightnessSpinner-sc-1yk0pqp-3 sLeDt"><button title="Mudar para modo diurno" aria-label="Mudar para modo diurno" class="Navigation__ClearButton-sc-1yk0pqp-4 Navigation__BrightnessIcon-sc-1yk0pqp-5 frsARl">🌙</button><button title="Mudar para modo noturno" aria-label="Mudar para modo noturno" class="Navigation__ClearButton-sc-1yk0pqp-4 Navigation__BrightnessIcon-sc-1yk0pqp-5 frsARl">☀️</button></div></div></div></nav><main><div class="ui__Fit-prwz6b-0 CfxHf"><div><div style="width:0%;opacity:0" class="post__ScrollTrackerContainer-gh0jqu-0 jjKdZc"></div><article class="Poststyles__Container-sc-12qb97v-4 kyWzPB"><h1 class="Poststyles__Title-sc-12qb97v-5 hhBwVH"><a href="/montando-5-graficos-com-uma-metrica-do-prometheus/">Montando 5 gráficos com uma métrica do Prometheus</a></h1><p class="Poststyles__Meta-sc-12qb97v-6 fonWHZ">Publicado em <!-- -->20 ago 2020<!-- -->. Uns <!-- -->10<!-- --> minutos de leitura.</p><div class="Poststyles__Content-sc-12qb97v-7 jhgzPU content"><p>A linguagem de consultas do Prometheus, a <strong>PromQL</strong>, permite fazer transformações e agregações das métricas para extrair dados que não haviam sido reportados diretamente pela aplicação mas podem ser deduzidos. Nesse post, trago 5 consultas diferentes que podemos fazer para monitorar nossas aplicações usando apenas <em>(tecnicamente)</em> uma métrica! Vamos usá-las para montar gráficos no Grafana.</p><span><!-- summary-break --></span><p>Eu digo <em>tecnicamente</em> pois vamos usar uma métrica do tipo <strong>histograma</strong>, que é um <a href="/tipos-complexos-de-metricas-no-prometheus/">tipo complexo de métrica do Prometheus</a>. Por baixo, um histograma é implementado usando várias séries temporais distintas, mas a biblioteca de cliente do Prometheus abstrai isso e a aplicação só precisa registrar cada evento em uma única métrica.</p><p>A métrica que vamos usar vai registrar a distribuição da latência das requisições HTTP que a aplicação recebe, em segundos. Vamos chamá-la de <code>http_request_duration_seconds</code>. Para conseguirmos fazer algumas agregações que queremos, vamos adicionar duas dimensões a essta métrica: <code>path</code> que é o caminho (ou endpoint) da requisição e <code>status_class</code> que é que tipo retorno foi dado a essa requisição, como <code>2XX</code>, <code>4XX</code>, <code>5XX</code>, e por aí vai.</p><p>A criação e registro dos eventos dessa métrica depende da linguagem e framework que você esteja usando. Alguns frameworks possuem integração com a biblioteca de cliente do Prometheus para a linguagem e podem já fornecer uma métrica como essa. Do contrário, você precisa observar as requisições que chegam e reportar uma métrica de histograma passando como valor o número de segundos que ela levou para terminar e, como dimensões, pelo menos as duas acima.</p><p>Sobre o <code>path</code>, é importante lembrar que dimensões (labels) no Prometheus não podem ter valores ilimitados, pois isso gera uma carga considerável nele. Uma regra de ouro é: se você não consegue listar todas as possibilidades de valores pra uma dimensão, ela não deveria existir. Para o <code>path</code>, isso quer dizer que temos que tomar cuidado com <strong>interpolações</strong>. Por exemplo, se a aplicação tem uma rota <code>/api/users/812376/profile</code>, em que <code>812376</code> representa um ID de usuário, devemos reportar o caminho sem interpolações, como <code>/api/users/:id/profile</code>.</p><p>Sobre o <code>status_class</code>, ele é uma abstração em cima do código de status da resposta da nossa aplicação. Por exemplo, se a aplicação retornar 200 ou 201, o <code>status_class</code> seria <code>2XX</code>. Se retornar 500 ou 503, seria <code>5XX</code>. Isso vai facilitar, por exemplo, filtrarmos todas as requisições em que o servidor deu um erro (<code>5XX</code>) sem ter que listar todos os possíveis códigos de erro HTTP.</p><p>Duas observações antes de começar: algumas dessas métricas, medidas do ponto de vista da aplicação, serão naturalmente enviesadas pois considerarão apenas requisições que <strong>chegaram a aplicação e foram respondidas</strong> (mesmo que com um erro 500). Porém, se a aplicação estiver mal das pernas, ela pode começar a derrubar conexões, ficar indisponível para reportar métricas, ou ainda um balanceador de carga que sirva a aplicação comece a enfileirar requisições e eventualmente desistr delas. Se você conseguir métricas assim do load balancer, pode ter uma figura mais verídica nos momento de maior aperto.</p><p>Similarmente, se você conseguir <strong>coletar métricas no cliente</strong> (navegador, aplicativo, etc), você consegue considerar falhas em requisições por razões como problemas de DNS, que nem chegariam no seu load balancer. É também possível ver latência real incluindo o tempo gasto pela conexão de rede do usuário, o que é interessante de analisar pois, mesmo que você não consiga influenciar muito a velocidade de conexão dessa pessoa usando 3G com um pontinho de sinal, entender esse comportamento pode ajudar a otimizar seu conteúdo para as condições mais típicas do seu usuário.</p><h2>Percentis de latência</h2><p>Começando por uma das aplicações mais comuns para histogramas no Prometheus: estimar percentis. Para isso, usamos a função <code>histogram_quantile</code> que espera um número de 0 a 1 (por exemplo, 0.5 é o percentil 50, ou mediana).</p><p>Para esta métrica, queremos ver os percentis de latência para a aplicação toda, independentemente do endpoint ou código de retorno. Então vamos agrupar nossa série temporal usando <code>sum</code>. Sempre que agregamos séries que vão ser usadas para a função <code>histogram_quantile</code>, precisamos manter a label <code>le</code> (mais sobre isso no <a href="/tipos-complexos-de-metricas-no-prometheus/">post sobre tipos complexos de métricas</a>).</p><pre class="refractor language-js"><code class="language-js"><span class="token function">histogram_quantile</span><span class="token punctuation">(</span><span class="token number">0.99</span><span class="token punctuation">,</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_bucket<span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token function">by</span> <span class="token punctuation">(</span>le<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class="refractor language-js"><code class="language-js"><span class="token function">histogram_quantile</span><span class="token punctuation">(</span><span class="token number">0.95</span><span class="token punctuation">,</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_bucket<span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token function">by</span> <span class="token punctuation">(</span>le<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><pre class="refractor language-js"><code class="language-js"><span class="token function">histogram_quantile</span><span class="token punctuation">(</span><span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_bucket<span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token function">by</span> <span class="token punctuation">(</span>le<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p><img src="/media/uma-metrica-grafico-1.png" alt="Gráfico de percentis de latência ao longo do tempo, com três linhas, cada uma representando um percentil (99, 90 e 50)"/></p><p>Cada consulta acima vai retornar uma série que representa o percentil de latência (99, 90 e 50, respectivamente). Como para o cálculo precisamos usar a função <code>rate</code>, precisamos de uma janela para o cálculo da métrica. Em todos os exemplos deste post vou usar 5 minutos. Esse valor tem que ser, no mínimo, maior que o dobro do intervalo entre coletas de métrica do Prometheus  (<code>scrape_interval</code> na configuração), pois você precisa de dois pontos de dados para calcular a variação.</p><h2>Latência média</h2><p>Métricas de histograma incluem, além das séries temporais dos buckets, duas séries a mais que contém a soma de todos os valores e a quantidade de valores registrados. Podemos usar essas séries para pegar a média aritimética dos valores. A média pode ser fortemente influenciada por valores extremos, e a análise de percentis é útil para evitar essa influência. Mas, se quisermos uma média aritimética, podemos obetê-la:</p><pre class="refractor language-js"><code class="language-js"><span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_sum<span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">/</span>
<span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_count<span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p><img src="/media/uma-metrica-grafico-2.png" alt="Gráfico de latência média ao longo do tempo, com uma linha"/></p><p>Para fazer essa consulta, além de dividir a soma pelo total (que é a definição de média aritimética), estamos usando a função <code>rate</code>. Ela serve neste caso para que nosso gráfico não considere <strong>todas os eventos da história</strong>. Sem o <code>rate</code>, vamos calcular a latência média desde que a aplicação iniciou, e não uma média &quot;instantânea&quot; (que não é instantânea de fato pois precisamos de uma janela para calcular a média, neste exemplo, de 5 minutos).</p><p>Com o <code>rate</code>, vamos pegar o quanto a soma dos valores cresceu nos últimos 5 minutos e dividir por o quanto a quatidade de eventos cresceu nos últimos 5 minutos. Assim, cada ponto da série temporal resultante é a latência média dos últimos 5 minutos.</p><h2>Disponibilidade (em sucessos/total)</h2><p>Dependendo do conceito de disponibilidade que você utiliza, a maneira que construimos essa métrica serve para fazer uma medição de disponibilidade da sua aplicação. Uma das definições, citada no <a href="https://landing.google.com/sre/sre-book/chapters/embracing-risk/">livro de SRE do Google</a>, é a de <strong>disponibilidade agregada</strong>. Ela parte da ideia de que o uptime (tempo em que o serviço estava disponível) pode ser menos relevante do que a quantidade de requisições com sucesso que a aplicação atendeu.</p><p>Se uma árvore cai numa floresta deserta e ninguém escuta, ela faz barulho? Se a sua aplicação passa 5 minutos offline mas ninguém tentou interagir com ela na quele momento, isso importa? Ficar 5 minutos fora do ar às 3 da manhã e ao meio-dia para uma aplicação de delivery de comida são situações bem diferentes.</p><p>Dado que nossa métrica tem uma dimensão de <code>status_class</code> e uma série temporal que representa a quantidade de requisições que aconteceram (a <code>_count</code>), podemos calcular o percentual de disponibilidade instantânea dela dividindo a quantidade de erros pelo total de requisições. Como queremos saber o percentual de requisições que não deu erro, vamos pegar como resultado <code>1 - percentual-de-erros</code>.</p><pre class="refractor language-js"><code class="language-js"><span class="token number">1</span> <span class="token operator">-</span>
<span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_count<span class="token punctuation">{</span>status_class<span class="token operator">=</span><span class="token string">&quot;5XX&quot;</span><span class="token punctuation">}</span><span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">/</span>
<span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_count<span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p><img src="/media/uma-metrica-grafico-3.png" alt="Gráfico de disponibilidade percentual ao longo do tempo. Majoritariamente em 100%, e em alguns momentos a disponibilidade desce até 96% e volta a subir"/></p><p>Podemos usar janelas maiores, que mais fazem sentido olhando números do que gráficos. Podemos ter um painel no Grafana representando, por exemplo, a disponibilidade nas últimas 24 horas mudando apenas o tamanho da janela.</p><pre class="refractor language-js"><code class="language-js"><span class="token number">1</span> <span class="token operator">-</span>
<span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_count<span class="token punctuation">{</span>status_class<span class="token operator">=</span><span class="token string">&quot;5XX&quot;</span><span class="token punctuation">}</span><span class="token punctuation">[</span><span class="token number">24</span>h<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token operator">/</span>
<span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_count<span class="token punctuation">[</span><span class="token number">24</span>h<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p><img src="/media/uma-metrica-painel.png" alt="Painel de disponibilidade das últimas 24 horas mostrando o número 99,873%"/></p><h2>Top endpoints mais problemáticos (percentual)</h2><p>Podemos reutilizar algumas consultas que fizemos acima, com uma agregação por <code>path</code>, para comparar a performance de diferentes endpoints da aplicação. Numa aplicação muito grande, a lista de endpoints pode ser muito extensa (mesmo tomando cuidado para não inserir interpolações nela), e podemos usar a função <code>topk</code> para diminuir essa lista. Como o nome diz, ela retorna os <code>k</code> maiores valores.</p><p>Vamos usar a mesma consulta do terceiro exemplo, mas adicionando um agrupamento da agregação de soma, <code>by (path)</code>, para termos um percentual de falha para cada endpoint e, com o <code>topk</code>, pegar os 10 maiores percentuais.</p><pre class="refractor language-js"><code class="language-js"><span class="token function">topk</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> 
  <span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_count<span class="token punctuation">{</span>status_class<span class="token operator">=</span><span class="token string">&quot;5XX&quot;</span><span class="token punctuation">}</span><span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token function">by</span> <span class="token punctuation">(</span>path<span class="token punctuation">)</span>
  <span class="token operator">/</span>
  <span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_count<span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token function">by</span> <span class="token punctuation">(</span>path<span class="token punctuation">)</span>
<span class="token punctuation">)</span></code></pre><p><img src="/media/uma-metrica-grafico-4.png" alt="Gráfico mostrando percentual de erros ao longo do tempo. Há várias linhas, uma para cada endpoint da aplicação que retornou algum erro durante a janela de tempo do gráfico. As linhas ficam em 0 na maior parte do tempo, com subidas ocasionais até 3%"/></p><p>Neste exemplo, usamos essa função para montar um gráfico. Note que, neste caso, ela pode retornar <strong>mais do que <code>k</code> séries</strong> pois, em algum momento do gráfico, uma série pode deixar de existir pois seu valor não está mais entre os <code>k</code> primeiros e, similarmente, novas séries podem surgir. Você pode também formatar essa consulta como uma <strong>tabela</strong> e olhar apenas para o valor mais recente, que certamente vai conter (no máximo) <code>k</code> linhas. No exemplo abaixo, apenas dois endpoints estavam reportando algum erro nos últimos 5 minutos e, por isso, só há duas linhas na tabela:</p><p><img src="/media/uma-metrica-tabela.png" alt="Tabela mostrando percentual de erros por endpoint da aplicação. Há duas linhas, uma para cada path que teve um erro (primeira coluna), e o valor do percentual de erros (segunda coluna)"/></p><h2>Top endpoints mais lentos (percentil)</h2><p>Seguindo a mesma lógica do exemplo anterior, mas usando a consulta do primeiro exemplo, podemos pegar a lista dos 10 endpoints com os maiores percentis-99 de latência. O Grafana também permite adicionar valores nas legendas. No gráfico abaixo, eu coloquei para mostrar o valor mais recente e ordernar a legenda por esse valor, mostrando os endpoints mais lentos no momento, em ordem, na legenda.</p><pre class="refractor language-js"><code class="language-js"><span class="token function">topk</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> 
  <span class="token function">histogram_quantile</span><span class="token punctuation">(</span><span class="token number">0.99</span><span class="token punctuation">,</span> <span class="token function">sum</span><span class="token punctuation">(</span><span class="token function">rate</span><span class="token punctuation">(</span>http_request_duration_seconds_bucket<span class="token punctuation">[</span><span class="token number">5</span>m<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token function">by</span> <span class="token punctuation">(</span>le<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span></code></pre><p><img src="/media/uma-metrica-grafico-5.png" alt="Gráfico de percentil 99 de latência ao longo do tempo. Há várias linhas, uma para cada endpoint da aplicação. Um endpoint é bem mais lento que os outros, com latências entre 3 e 15 segundos. Os outros estão na casa das centenas ou dezenas de milissegundos"/></p><p>Lembrando que é importante manter a dimensão <code>le</code> na agregação sempre que formos usá-la dentro de um <code>histogram_quantile</code>.</p><p>Assim como no exemplo anterior, montar um gráfico usando <code>topk</code> pode retornar mais de <code>k</code> séries. Além de usar uma tabela, é possível montar um gráfico evitando mostrar mais de <code>k</code> séries isso, tomando uma decisão de quais séries retornar, em um momento do tempo, e depois consultando apenas essas séries. Há um exemplo disso <a href="https://www.robustperception.io/graph-top-n-time-series-in-grafana">neste post da Robust Perception</a>.</p><hr/><p>Pra fechar, vale comentar que a gente <strong>não precisa</strong> ter apenas uma métrica na aplicação. É perfeitamente aceitável criar uma métrica para medir o histograma das latências e uma outra para medir o número de requisições, com separação por status de retorno e outras dimensões. Para esse post, eu me aproveitei de que todo histograma traz consigo um contador de ocorrências e adicionei as labels que precisávamos para montar os gráficos acima.</p><p>Essas consultas podem ser usadas não só para montar gráficos bonitos no Grafana, mas também para criar alertas usando o <a href="https://prometheus.io/docs/alerting/latest/alertmanager/">Alert Manager</a>, componente do Prometheus que roda consultas nele e gera alertas caso essas consultas retornem algum estado em particular. Quais métricas usar para alertar e como definir bons alertas são assuntos interessantes para tratar em posts futuros! :)</p></div><hr/><div class="Poststyles__AfterPost-sc-12qb97v-0 tuaWG"><p class="Poststyles__AfterPostChild-sc-12qb97v-1 gkeqUt">Gostou? <a href="https://twitter.com/intent/tweet?text=%22Montando%205%20gr%C3%A1ficos%20com%20uma%20m%C3%A9trica%20do%20Prometheus%22%20por%20%40Cisneiros%0A%0Ahttps%3A%2F%2Fblog.cisne.dev%2Fmontando-5-graficos-com-uma-metrica-do-prometheus" target="_blank" rel="noopener noreferrer">Que tal compartilhar?</a></p><p class="Poststyles__AfterPostChild-sc-12qb97v-1 Poststyles__Tags-sc-12qb97v-2 fAvgXR">Tags: <span class="Poststyles__Tag-sc-12qb97v-3 kSZCZX"><a href="/tag/prometheus">prometheus</a></span><span class="Poststyles__Tag-sc-12qb97v-3 kSZCZX"><a href="/tag/monitoring">monitoring</a></span><span class="Poststyles__Tag-sc-12qb97v-3 kSZCZX"><a href="/tag/grafana">grafana</a></span></p></div></article></div></div></main><footer class="Footer__Container-b3q04c-0 gdHjSq"><div class="ui__Fit-prwz6b-0 Footer__FooterFit-b3q04c-1 layJcq"><p>© <a href="https://cisne.dev">Alexandre Cisneiros</a></p><p><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">CC-BY-SA 4.0</a></p></div></footer></div><script type="text/javascript">window.__routeInfo = JSON.parse("{\"template\":\"__react_static_root__/src/containers/post\",\"sharedHashesByProp\":{},\"data\":{\"postData\":{\"title\":\"Montando 5 gr\u00E1ficos com uma m\u00E9trica do Prometheus\",\"date\":\"2020-08-20T00:00:00-03:00\",\"slug\":\"montando-5-graficos-com-uma-metrica-do-prometheus\",\"tags\":[\"prometheus\",\"monitoring\",\"grafana\"],\"filePath\":\"2020-08-20-montando-5-graficos-com-uma-metrica-do-prometheus.md\",\"content\":\"A linguagem de consultas do Prometheus, a **PromQL**, permite fazer transforma\u00E7\u00F5es e agrega\u00E7\u00F5es das m\u00E9tricas para extrair dados que n\u00E3o haviam sido reportados diretamente pela aplica\u00E7\u00E3o mas podem ser deduzidos. Nesse post, trago 5 consultas diferentes que podemos fazer para monitorar nossas aplica\u00E7\u00F5es usando apenas _(tecnicamente)_ uma m\u00E9trica! Vamos us\u00E1-las para montar gr\u00E1ficos no Grafana.\\n\\n\u003C!-- summary-break -->\\n\\nEu digo _tecnicamente_ pois vamos usar uma m\u00E9trica do tipo **histograma**, que \u00E9 um [tipo complexo de m\u00E9trica do Prometheus](/tipos-complexos-de-metricas-no-prometheus/). Por baixo, um histograma \u00E9 implementado usando v\u00E1rias s\u00E9ries temporais distintas, mas a biblioteca de cliente do Prometheus abstrai isso e a aplica\u00E7\u00E3o s\u00F3 precisa registrar cada evento em uma \u00FAnica m\u00E9trica.\\n\\nA m\u00E9trica que vamos usar vai registrar a distribui\u00E7\u00E3o da lat\u00EAncia das requisi\u00E7\u00F5es HTTP que a aplica\u00E7\u00E3o recebe, em segundos. Vamos cham\u00E1-la de `http_request_duration_seconds`. Para conseguirmos fazer algumas agrega\u00E7\u00F5es que queremos, vamos adicionar duas dimens\u00F5es a essta m\u00E9trica: `path` que \u00E9 o caminho (ou endpoint) da requisi\u00E7\u00E3o e `status_class` que \u00E9 que tipo retorno foi dado a essa requisi\u00E7\u00E3o, como `2XX`, `4XX`, `5XX`, e por a\u00ED vai.\\n\\nA cria\u00E7\u00E3o e registro dos eventos dessa m\u00E9trica depende da linguagem e framework que voc\u00EA esteja usando. Alguns frameworks possuem integra\u00E7\u00E3o com a biblioteca de cliente do Prometheus para a linguagem e podem j\u00E1 fornecer uma m\u00E9trica como essa. Do contr\u00E1rio, voc\u00EA precisa observar as requisi\u00E7\u00F5es que chegam e reportar uma m\u00E9trica de histograma passando como valor o n\u00FAmero de segundos que ela levou para terminar e, como dimens\u00F5es, pelo menos as duas acima.\\n\\nSobre o `path`, \u00E9 importante lembrar que dimens\u00F5es (labels) no Prometheus n\u00E3o podem ter valores ilimitados, pois isso gera uma carga consider\u00E1vel nele. Uma regra de ouro \u00E9: se voc\u00EA n\u00E3o consegue listar todas as possibilidades de valores pra uma dimens\u00E3o, ela n\u00E3o deveria existir. Para o `path`, isso quer dizer que temos que tomar cuidado com **interpola\u00E7\u00F5es**. Por exemplo, se a aplica\u00E7\u00E3o tem uma rota `/api/users/812376/profile`, em que `812376` representa um ID de usu\u00E1rio, devemos reportar o caminho sem interpola\u00E7\u00F5es, como `/api/users/:id/profile`.\\n\\nSobre o `status_class`, ele \u00E9 uma abstra\u00E7\u00E3o em cima do c\u00F3digo de status da resposta da nossa aplica\u00E7\u00E3o. Por exemplo, se a aplica\u00E7\u00E3o retornar 200 ou 201, o `status_class` seria `2XX`. Se retornar 500 ou 503, seria `5XX`. Isso vai facilitar, por exemplo, filtrarmos todas as requisi\u00E7\u00F5es em que o servidor deu um erro (`5XX`) sem ter que listar todos os poss\u00EDveis c\u00F3digos de erro HTTP.\\n\\nDuas observa\u00E7\u00F5es antes de come\u00E7ar: algumas dessas m\u00E9tricas, medidas do ponto de vista da aplica\u00E7\u00E3o, ser\u00E3o naturalmente enviesadas pois considerar\u00E3o apenas requisi\u00E7\u00F5es que **chegaram a aplica\u00E7\u00E3o e foram respondidas** (mesmo que com um erro 500). Por\u00E9m, se a aplica\u00E7\u00E3o estiver mal das pernas, ela pode come\u00E7ar a derrubar conex\u00F5es, ficar indispon\u00EDvel para reportar m\u00E9tricas, ou ainda um balanceador de carga que sirva a aplica\u00E7\u00E3o comece a enfileirar requisi\u00E7\u00F5es e eventualmente desistr delas. Se voc\u00EA conseguir m\u00E9tricas assim do load balancer, pode ter uma figura mais ver\u00EDdica nos momento de maior aperto.\\n\\nSimilarmente, se voc\u00EA conseguir **coletar m\u00E9tricas no cliente** (navegador, aplicativo, etc), voc\u00EA consegue considerar falhas em requisi\u00E7\u00F5es por raz\u00F5es como problemas de DNS, que nem chegariam no seu load balancer. \u00C9 tamb\u00E9m poss\u00EDvel ver lat\u00EAncia real incluindo o tempo gasto pela conex\u00E3o de rede do usu\u00E1rio, o que \u00E9 interessante de analisar pois, mesmo que voc\u00EA n\u00E3o consiga influenciar muito a velocidade de conex\u00E3o dessa pessoa usando 3G com um pontinho de sinal, entender esse comportamento pode ajudar a otimizar seu conte\u00FAdo para as condi\u00E7\u00F5es mais t\u00EDpicas do seu usu\u00E1rio.\\n\\n## Percentis de lat\u00EAncia\\n\\nCome\u00E7ando por uma das aplica\u00E7\u00F5es mais comuns para histogramas no Prometheus: estimar percentis. Para isso, usamos a fun\u00E7\u00E3o `histogram_quantile` que espera um n\u00FAmero de 0 a 1 (por exemplo, 0.5 \u00E9 o percentil 50, ou mediana).\\n\\nPara esta m\u00E9trica, queremos ver os percentis de lat\u00EAncia para a aplica\u00E7\u00E3o toda, independentemente do endpoint ou c\u00F3digo de retorno. Ent\u00E3o vamos agrupar nossa s\u00E9rie temporal usando `sum`. Sempre que agregamos s\u00E9ries que v\u00E3o ser usadas para a fun\u00E7\u00E3o `histogram_quantile`, precisamos manter a label `le` (mais sobre isso no [post sobre tipos complexos de m\u00E9tricas](/tipos-complexos-de-metricas-no-prometheus/)).\\n\\n```promql\\nhistogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))\\n```\\n```promql\\nhistogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))\\n```\\n```promql\\nhistogram_quantile(0.5, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))\\n```\\n\\n![Gr\u00E1fico de percentis de lat\u00EAncia ao longo do tempo, com tr\u00EAs linhas, cada uma representando um percentil (99, 90 e 50)](/media/uma-metrica-grafico-1.png)\\n\\nCada consulta acima vai retornar uma s\u00E9rie que representa o percentil de lat\u00EAncia (99, 90 e 50, respectivamente). Como para o c\u00E1lculo precisamos usar a fun\u00E7\u00E3o `rate`, precisamos de uma janela para o c\u00E1lculo da m\u00E9trica. Em todos os exemplos deste post vou usar 5 minutos. Esse valor tem que ser, no m\u00EDnimo, maior que o dobro do intervalo entre coletas de m\u00E9trica do Prometheus  (`scrape_interval` na configura\u00E7\u00E3o), pois voc\u00EA precisa de dois pontos de dados para calcular a varia\u00E7\u00E3o.\\n\\n## Lat\u00EAncia m\u00E9dia\\n\\nM\u00E9tricas de histograma incluem, al\u00E9m das s\u00E9ries temporais dos buckets, duas s\u00E9ries a mais que cont\u00E9m a soma de todos os valores e a quantidade de valores registrados. Podemos usar essas s\u00E9ries para pegar a m\u00E9dia aritim\u00E9tica dos valores. A m\u00E9dia pode ser fortemente influenciada por valores extremos, e a an\u00E1lise de percentis \u00E9 \u00FAtil para evitar essa influ\u00EAncia. Mas, se quisermos uma m\u00E9dia aritim\u00E9tica, podemos obet\u00EA-la:\\n\\n```promql\\nsum(rate(http_request_duration_seconds_sum[5m]))\\n/\\nsum(rate(http_request_duration_seconds_count[5m]))\\n```\\n\\n![Gr\u00E1fico de lat\u00EAncia m\u00E9dia ao longo do tempo, com uma linha](/media/uma-metrica-grafico-2.png)\\n\\nPara fazer essa consulta, al\u00E9m de dividir a soma pelo total (que \u00E9 a defini\u00E7\u00E3o de m\u00E9dia aritim\u00E9tica), estamos usando a fun\u00E7\u00E3o `rate`. Ela serve neste caso para que nosso gr\u00E1fico n\u00E3o considere **todas os eventos da hist\u00F3ria**. Sem o `rate`, vamos calcular a lat\u00EAncia m\u00E9dia desde que a aplica\u00E7\u00E3o iniciou, e n\u00E3o uma m\u00E9dia \\\"instant\u00E2nea\\\" (que n\u00E3o \u00E9 instant\u00E2nea de fato pois precisamos de uma janela para calcular a m\u00E9dia, neste exemplo, de 5 minutos).\\n\\nCom o `rate`, vamos pegar o quanto a soma dos valores cresceu nos \u00FAltimos 5 minutos e dividir por o quanto a quatidade de eventos cresceu nos \u00FAltimos 5 minutos. Assim, cada ponto da s\u00E9rie temporal resultante \u00E9 a lat\u00EAncia m\u00E9dia dos \u00FAltimos 5 minutos.\\n\\n## Disponibilidade (em sucessos/total)\\n\\nDependendo do conceito de disponibilidade que voc\u00EA utiliza, a maneira que construimos essa m\u00E9trica serve para fazer uma medi\u00E7\u00E3o de disponibilidade da sua aplica\u00E7\u00E3o. Uma das defini\u00E7\u00F5es, citada no [livro de SRE do Google](https://landing.google.com/sre/sre-book/chapters/embracing-risk/), \u00E9 a de **disponibilidade agregada**. Ela parte da ideia de que o uptime (tempo em que o servi\u00E7o estava dispon\u00EDvel) pode ser menos relevante do que a quantidade de requisi\u00E7\u00F5es com sucesso que a aplica\u00E7\u00E3o atendeu.\\n\\nSe uma \u00E1rvore cai numa floresta deserta e ningu\u00E9m escuta, ela faz barulho? Se a sua aplica\u00E7\u00E3o passa 5 minutos offline mas ningu\u00E9m tentou interagir com ela na quele momento, isso importa? Ficar 5 minutos fora do ar \u00E0s 3 da manh\u00E3 e ao meio-dia para uma aplica\u00E7\u00E3o de delivery de comida s\u00E3o situa\u00E7\u00F5es bem diferentes.\\n\\nDado que nossa m\u00E9trica tem uma dimens\u00E3o de `status_class` e uma s\u00E9rie temporal que representa a quantidade de requisi\u00E7\u00F5es que aconteceram (a `_count`), podemos calcular o percentual de disponibilidade instant\u00E2nea dela dividindo a quantidade de erros pelo total de requisi\u00E7\u00F5es. Como queremos saber o percentual de requisi\u00E7\u00F5es que n\u00E3o deu erro, vamos pegar como resultado `1 - percentual-de-erros`.\\n\\n```promql\\n1 -\\nsum(rate(http_request_duration_seconds_count{status_class=\\\"5XX\\\"}[5m]))\\n/\\nsum(rate(http_request_duration_seconds_count[5m]))\\n```\\n\\n![Gr\u00E1fico de disponibilidade percentual ao longo do tempo. Majoritariamente em 100%, e em alguns momentos a disponibilidade desce at\u00E9 96% e volta a subir](/media/uma-metrica-grafico-3.png)\\n\\nPodemos usar janelas maiores, que mais fazem sentido olhando n\u00FAmeros do que gr\u00E1ficos. Podemos ter um painel no Grafana representando, por exemplo, a disponibilidade nas \u00FAltimas 24 horas mudando apenas o tamanho da janela.\\n\\n```promql\\n1 -\\nsum(rate(http_request_duration_seconds_count{status_class=\\\"5XX\\\"}[24h]))\\n/\\nsum(rate(http_request_duration_seconds_count[24h]))\\n```\\n\\n![Painel de disponibilidade das \u00FAltimas 24 horas mostrando o n\u00FAmero 99,873%](/media/uma-metrica-painel.png)\\n\\n## Top endpoints mais problem\u00E1ticos (percentual)\\n\\nPodemos reutilizar algumas consultas que fizemos acima, com uma agrega\u00E7\u00E3o por `path`, para comparar a performance de diferentes endpoints da aplica\u00E7\u00E3o. Numa aplica\u00E7\u00E3o muito grande, a lista de endpoints pode ser muito extensa (mesmo tomando cuidado para n\u00E3o inserir interpola\u00E7\u00F5es nela), e podemos usar a fun\u00E7\u00E3o `topk` para diminuir essa lista. Como o nome diz, ela retorna os `k` maiores valores.\\n\\nVamos usar a mesma consulta do terceiro exemplo, mas adicionando um agrupamento da agrega\u00E7\u00E3o de soma, `by (path)`, para termos um percentual de falha para cada endpoint e, com o `topk`, pegar os 10 maiores percentuais.\\n\\n```promql\\ntopk(10, \\n  sum(rate(http_request_duration_seconds_count{status_class=\\\"5XX\\\"}[5m])) by (path)\\n  /\\n  sum(rate(http_request_duration_seconds_count[5m])) by (path)\\n)\\n```\\n\\n![Gr\u00E1fico mostrando percentual de erros ao longo do tempo. H\u00E1 v\u00E1rias linhas, uma para cada endpoint da aplica\u00E7\u00E3o que retornou algum erro durante a janela de tempo do gr\u00E1fico. As linhas ficam em 0 na maior parte do tempo, com subidas ocasionais at\u00E9 3%](/media/uma-metrica-grafico-4.png)\\n\\nNeste exemplo, usamos essa fun\u00E7\u00E3o para montar um gr\u00E1fico. Note que, neste caso, ela pode retornar **mais do que `k` s\u00E9ries** pois, em algum momento do gr\u00E1fico, uma s\u00E9rie pode deixar de existir pois seu valor n\u00E3o est\u00E1 mais entre os `k` primeiros e, similarmente, novas s\u00E9ries podem surgir. Voc\u00EA pode tamb\u00E9m formatar essa consulta como uma **tabela** e olhar apenas para o valor mais recente, que certamente vai conter (no m\u00E1ximo) `k` linhas. No exemplo abaixo, apenas dois endpoints estavam reportando algum erro nos \u00FAltimos 5 minutos e, por isso, s\u00F3 h\u00E1 duas linhas na tabela:\\n\\n![Tabela mostrando percentual de erros por endpoint da aplica\u00E7\u00E3o. H\u00E1 duas linhas, uma para cada path que teve um erro (primeira coluna), e o valor do percentual de erros (segunda coluna)](/media/uma-metrica-tabela.png)\\n\\n\\n## Top endpoints mais lentos (percentil)\\n\\nSeguindo a mesma l\u00F3gica do exemplo anterior, mas usando a consulta do primeiro exemplo, podemos pegar a lista dos 10 endpoints com os maiores percentis-99 de lat\u00EAncia. O Grafana tamb\u00E9m permite adicionar valores nas legendas. No gr\u00E1fico abaixo, eu coloquei para mostrar o valor mais recente e ordernar a legenda por esse valor, mostrando os endpoints mais lentos no momento, em ordem, na legenda.\\n\\n```promql\\ntopk(10, \\n  histogram_quantile(0.99, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, path))\\n)\\n```\\n\\n![Gr\u00E1fico de percentil 99 de lat\u00EAncia ao longo do tempo. H\u00E1 v\u00E1rias linhas, uma para cada endpoint da aplica\u00E7\u00E3o. Um endpoint \u00E9 bem mais lento que os outros, com lat\u00EAncias entre 3 e 15 segundos. Os outros est\u00E3o na casa das centenas ou dezenas de milissegundos](/media/uma-metrica-grafico-5.png)\\n\\nLembrando que \u00E9 importante manter a dimens\u00E3o `le` na agrega\u00E7\u00E3o sempre que formos us\u00E1-la dentro de um `histogram_quantile`.\\n\\nAssim como no exemplo anterior, montar um gr\u00E1fico usando `topk` pode retornar mais de `k` s\u00E9ries. Al\u00E9m de usar uma tabela, \u00E9 poss\u00EDvel montar um gr\u00E1fico evitando mostrar mais de `k` s\u00E9ries isso, tomando uma decis\u00E3o de quais s\u00E9ries retornar, em um momento do tempo, e depois consultando apenas essas s\u00E9ries. H\u00E1 um exemplo disso [neste post da Robust Perception](https://www.robustperception.io/graph-top-n-time-series-in-grafana).\\n\\n---\\n\\nPra fechar, vale comentar que a gente **n\u00E3o precisa** ter apenas uma m\u00E9trica na aplica\u00E7\u00E3o. \u00C9 perfeitamente aceit\u00E1vel criar uma m\u00E9trica para medir o histograma das lat\u00EAncias e uma outra para medir o n\u00FAmero de requisi\u00E7\u00F5es, com separa\u00E7\u00E3o por status de retorno e outras dimens\u00F5es. Para esse post, eu me aproveitei de que todo histograma traz consigo um contador de ocorr\u00EAncias e adicionei as labels que precis\u00E1vamos para montar os gr\u00E1ficos acima.\\n\\nEssas consultas podem ser usadas n\u00E3o s\u00F3 para montar gr\u00E1ficos bonitos no Grafana, mas tamb\u00E9m para criar alertas usando o [Alert Manager](https://prometheus.io/docs/alerting/latest/alertmanager/), componente do Prometheus que roda consultas nele e gera alertas caso essas consultas retornem algum estado em particular. Quais m\u00E9tricas usar para alertar e como definir bons alertas s\u00E3o assuntos interessantes para tratar em posts futuros! :)\\n\",\"readingTime\":10,\"summary\":\"A linguagem de consultas do Prometheus, a **PromQL**, permite fazer transforma\u00E7\u00F5es e agrega\u00E7\u00F5es das m\u00E9tricas para extrair dados que n\u00E3o haviam sido reportados diretamente pela aplica\u00E7\u00E3o mas podem ser deduzidos. Nesse post, trago 5 consultas diferentes que podemos fazer para monitorar nossas aplica\u00E7\u00F5es usando apenas _(tecnicamente)_ uma m\u00E9trica! Vamos us\u00E1-las para montar gr\u00E1ficos no Grafana.\\n\\n\"}},\"path\":\"montando-5-graficos-com-uma-metrica-do-prometheus\",\"sharedData\":{},\"siteData\":{}}");</script><script defer="" type="text/javascript" src="/templates/vendors~__react_static_root__/src/containers/post~__react_static_root__/src/containers/postList.207edc50.js"></script><script defer="" type="text/javascript" src="/templates/__react_static_root__/src/containers/post~__react_static_root__/src/containers/postList.a04ea84b.js"></script><script defer="" type="text/javascript" src="/templates/__react_static_root__/src/containers/post.d8dd52db.js"></script><script defer="" type="text/javascript" src="/templates/vendors~main.272ae1d1.js"></script><script defer="" type="text/javascript" src="/main.f19d4c14.js"></script></body></html>